{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_Lime.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "TS8AKdE3VdMN",
        "Lm2DO3TJF8CV"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tp-2n809FfAs"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FOUpZvLnqAR",
        "outputId": "618f6cd4-71b1-4dfb-a299-e2455479247e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dDUaMmIGcKl"
      },
      "source": [
        "# MNIST Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbnA9Z-eHqoi"
      },
      "source": [
        "EPOCHS = 10\n",
        "DEVICE = 'cuda'\n",
        "batch_size = 2048"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-fnc9XSVY_G"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VObEa3vPHtFa"
      },
      "source": [
        "trainloader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('./data', train=True, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size, shuffle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EfxCK0uMDZy"
      },
      "source": [
        "test_set =  torchvision.datasets.MNIST('./data', train=False, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ]))\n",
        "testloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QG1cVVibVW_t"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nnqa1bkQHuTy"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "net = Net()\n",
        "net = net.to(DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx4T-kpHHvnN"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS8AKdE3VdMN"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdrtYpBLHw1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a72f0c44-224d-4b47-dc97-42a33fa758f0"
      },
      "source": [
        "net.train()\n",
        "for epoch in range(EPOCHS):  # loop over the dataset multiple times\n",
        "    batch_len = len(trainloader)\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % int(batch_len/4) == int(batch_len/4) - 1:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / int(batch_len/4)))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1,     7] loss: 2.251\n",
            "[1,    14] loss: 2.041\n",
            "[1,    21] loss: 1.663\n",
            "[1,    28] loss: 1.254\n",
            "[2,     7] loss: 0.914\n",
            "[2,    14] loss: 0.762\n",
            "[2,    21] loss: 0.664\n",
            "[2,    28] loss: 0.577\n",
            "[3,     7] loss: 0.518\n",
            "[3,    14] loss: 0.471\n",
            "[3,    21] loss: 0.457\n",
            "[3,    28] loss: 0.418\n",
            "[4,     7] loss: 0.392\n",
            "[4,    14] loss: 0.381\n",
            "[4,    21] loss: 0.376\n",
            "[4,    28] loss: 0.355\n",
            "[5,     7] loss: 0.351\n",
            "[5,    14] loss: 0.335\n",
            "[5,    21] loss: 0.322\n",
            "[5,    28] loss: 0.307\n",
            "[6,     7] loss: 0.291\n",
            "[6,    14] loss: 0.294\n",
            "[6,    21] loss: 0.300\n",
            "[6,    28] loss: 0.269\n",
            "[7,     7] loss: 0.263\n",
            "[7,    14] loss: 0.262\n",
            "[7,    21] loss: 0.251\n",
            "[7,    28] loss: 0.264\n",
            "[8,     7] loss: 0.257\n",
            "[8,    14] loss: 0.239\n",
            "[8,    21] loss: 0.241\n",
            "[8,    28] loss: 0.234\n",
            "[9,     7] loss: 0.234\n",
            "[9,    14] loss: 0.227\n",
            "[9,    21] loss: 0.226\n",
            "[9,    28] loss: 0.217\n",
            "[10,     7] loss: 0.219\n",
            "[10,    14] loss: 0.212\n",
            "[10,    21] loss: 0.222\n",
            "[10,    28] loss: 0.214\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFltWCzPHyN8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6bbfe26-a8bc-4c7a-f7d1-641b6a00de80"
      },
      "source": [
        "net.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = net(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 97 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQx7AHcwnWVZ"
      },
      "source": [
        "## Save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjGBItG_nZVQ",
        "outputId": "a6ee0dd7-6f8b-4e4f-c962-980040bf8e8f"
      },
      "source": [
        "torch.save(net.state_dict(), './drive/My Drive/saved_models/MNIST_model')\n",
        "print('saved')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QuGwcu5lEUz"
      },
      "source": [
        "## Load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiF0X3XPlHf8",
        "outputId": "aa21b00a-972d-413a-9945-d98413289e63"
      },
      "source": [
        "net.load_state_dict(torch.load('./drive/My Drive/saved_models/MNIST_model'))\n",
        "net.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
              "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
              "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lm2DO3TJF8CV"
      },
      "source": [
        "# Lime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gf5Jkj_1HXg9"
      },
      "source": [
        "import sys\n",
        "import inspect\n",
        "def has_arg(fn, arg_name):\n",
        "\n",
        "    if sys.version_info < (3,):\n",
        "        if isinstance(fn, types.FunctionType) or isinstance(fn, types.MethodType):\n",
        "            arg_spec = inspect.getargspec(fn)\n",
        "        else:\n",
        "            try:\n",
        "                arg_spec = inspect.getargspec(fn.__call__)\n",
        "            except AttributeError:\n",
        "                return False\n",
        "        return (arg_name in arg_spec.args)\n",
        "    elif sys.version_info < (3, 6):\n",
        "        arg_spec = inspect.getfullargspec(fn)\n",
        "        return (arg_name in arg_spec.args or\n",
        "                arg_name in arg_spec.kwonlyargs)\n",
        "    else:\n",
        "        try:\n",
        "            signature = inspect.signature(fn)\n",
        "        except ValueError:\n",
        "            # handling Cython\n",
        "            signature = inspect.signature(fn.__call__)\n",
        "        parameter = signature.parameters.get(arg_name)\n",
        "        if parameter is None:\n",
        "            return False\n",
        "        return (parameter.kind in (inspect.Parameter.POSITIONAL_OR_KEYWORD,\n",
        "                                   inspect.Parameter.KEYWORD_ONLY))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4XiwTgVHMG8"
      },
      "source": [
        "import types\n",
        "\n",
        "from skimage.segmentation import felzenszwalb, slic, quickshift\n",
        "\n",
        "\n",
        "class BaseWrapper(object):\n",
        "\n",
        "    def __init__(self, target_fn=None, **target_params):\n",
        "        self.target_fn = target_fn\n",
        "        self.target_params = target_params\n",
        "\n",
        "    def _check_params(self, parameters):\n",
        "        a_valid_fn = []\n",
        "        if self.target_fn is None:\n",
        "            if callable(self):\n",
        "                a_valid_fn.append(self.__call__)\n",
        "            else:\n",
        "                raise TypeError('invalid argument: tested object is not callable,\\\n",
        "                 please provide a valid target_fn')\n",
        "        elif isinstance(self.target_fn, types.FunctionType) \\\n",
        "                or isinstance(self.target_fn, types.MethodType):\n",
        "            a_valid_fn.append(self.target_fn)\n",
        "        else:\n",
        "            a_valid_fn.append(self.target_fn.__call__)\n",
        "\n",
        "        if not isinstance(parameters, str):\n",
        "            for p in parameters:\n",
        "                for fn in a_valid_fn:\n",
        "                    if has_arg(fn, p):\n",
        "                        pass\n",
        "                    else:\n",
        "                        raise ValueError('{} is not a valid parameter'.format(p))\n",
        "        else:\n",
        "            raise TypeError('invalid argument: list or dictionnary expected')\n",
        "\n",
        "    def set_params(self, **params):\n",
        "\n",
        "        self._check_params(params)\n",
        "        self.target_params = params\n",
        "\n",
        "    def filter_params(self, fn, override=None):\n",
        "\n",
        "        override = override or {}\n",
        "        result = {}\n",
        "        for name, value in self.target_params.items():\n",
        "            if has_arg(fn, name):\n",
        "                result.update({name: value})\n",
        "        result.update(override)\n",
        "        return result\n",
        "\n",
        "\n",
        "class SegmentationAlgorithm(BaseWrapper):\n",
        "\n",
        "    def __init__(self, algo_type, **target_params):\n",
        "        self.algo_type = algo_type\n",
        "        if (self.algo_type == 'quickshift'):\n",
        "            BaseWrapper.__init__(self, quickshift, **target_params)\n",
        "            kwargs = self.filter_params(quickshift)\n",
        "            self.set_params(**kwargs)\n",
        "        elif (self.algo_type == 'felzenszwalb'):\n",
        "            BaseWrapper.__init__(self, felzenszwalb, **target_params)\n",
        "            kwargs = self.filter_params(felzenszwalb)\n",
        "            self.set_params(**kwargs)\n",
        "        elif (self.algo_type == 'slic'):\n",
        "            BaseWrapper.__init__(self, slic, **target_params)\n",
        "            kwargs = self.filter_params(slic)\n",
        "            self.set_params(**kwargs)\n",
        "\n",
        "    def __call__(self, *args):\n",
        "        return self.target_fn(args[0], **self.target_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSj_sk34GC4N"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "from sklearn.linear_model import Ridge, lars_path\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "\n",
        "class LimeBase(object):\n",
        "    def __init__(self,\n",
        "                 kernel_fn,\n",
        "                 verbose=False,\n",
        "                 random_state=None):\n",
        "\n",
        "        self.kernel_fn = kernel_fn\n",
        "        self.verbose = verbose\n",
        "        self.random_state = check_random_state(random_state)\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_lars_path(weighted_data, weighted_labels):\n",
        "\n",
        "        x_vector = weighted_data\n",
        "        alphas, _, coefs = lars_path(x_vector,\n",
        "                                     weighted_labels,\n",
        "                                     method='lasso',\n",
        "                                     verbose=False)\n",
        "        return alphas, coefs\n",
        "\n",
        "    def forward_selection(self, data, labels, weights, num_features):\n",
        "\n",
        "        clf = Ridge(alpha=0, fit_intercept=True, random_state=self.random_state)\n",
        "        used_features = []\n",
        "        for _ in range(min(num_features, data.shape[1])):\n",
        "            max_ = -100000000\n",
        "            best = 0\n",
        "            for feature in range(data.shape[1]):\n",
        "                if feature in used_features:\n",
        "                    continue\n",
        "                clf.fit(data[:, used_features + [feature]], labels,\n",
        "                        sample_weight=weights)\n",
        "                score = clf.score(data[:, used_features + [feature]],\n",
        "                                  labels,\n",
        "                                  sample_weight=weights)\n",
        "                if score > max_:\n",
        "                    best = feature\n",
        "                    max_ = score\n",
        "            used_features.append(best)\n",
        "        return np.array(used_features)\n",
        "\n",
        "    def feature_selection(self, data, labels, weights, num_features, method):\n",
        "        if method == 'none':\n",
        "            return np.array(range(data.shape[1]))\n",
        "        elif method == 'forward_selection':\n",
        "            return self.forward_selection(data, labels, weights, num_features)\n",
        "        elif method == 'highest_weights':\n",
        "            clf = Ridge(alpha=0.01, fit_intercept=True,\n",
        "                        random_state=self.random_state)\n",
        "            clf.fit(data, labels, sample_weight=weights)\n",
        "\n",
        "            coef = clf.coef_\n",
        "            if sp.sparse.issparse(data):\n",
        "                coef = sp.sparse.csr_matrix(clf.coef_)\n",
        "                weighted_data = coef.multiply(data[0])\n",
        "                # Note: most efficient to slice the data before reversing\n",
        "                sdata = len(weighted_data.data)\n",
        "                argsort_data = np.abs(weighted_data.data).argsort()\n",
        "                # Edge case where data is more sparse than requested number of feature importances\n",
        "                # In that case, we just pad with zero-valued features\n",
        "                if sdata < num_features:\n",
        "                    nnz_indexes = argsort_data[::-1]\n",
        "                    indices = weighted_data.indices[nnz_indexes]\n",
        "                    num_to_pad = num_features - sdata\n",
        "                    indices = np.concatenate((indices, np.zeros(num_to_pad, dtype=indices.dtype)))\n",
        "                    indices_set = set(indices)\n",
        "                    pad_counter = 0\n",
        "                    for i in range(data.shape[1]):\n",
        "                        if i not in indices_set:\n",
        "                            indices[pad_counter + sdata] = i\n",
        "                            pad_counter += 1\n",
        "                            if pad_counter >= num_to_pad:\n",
        "                                break\n",
        "                else:\n",
        "                    nnz_indexes = argsort_data[sdata - num_features:sdata][::-1]\n",
        "                    indices = weighted_data.indices[nnz_indexes]\n",
        "                return indices\n",
        "            else:\n",
        "                weighted_data = coef * data[0]\n",
        "                feature_weights = sorted(\n",
        "                    zip(range(data.shape[1]), weighted_data),\n",
        "                    key=lambda x: np.abs(x[1]),\n",
        "                    reverse=True)\n",
        "                return np.array([x[0] for x in feature_weights[:num_features]])\n",
        "        elif method == 'lasso_path':\n",
        "            weighted_data = ((data - np.average(data, axis=0, weights=weights))\n",
        "                             * np.sqrt(weights[:, np.newaxis]))\n",
        "            weighted_labels = ((labels - np.average(labels, weights=weights))\n",
        "                               * np.sqrt(weights))\n",
        "            nonzero = range(weighted_data.shape[1])\n",
        "            _, coefs = self.generate_lars_path(weighted_data,\n",
        "                                               weighted_labels)\n",
        "            for i in range(len(coefs.T) - 1, 0, -1):\n",
        "                nonzero = coefs.T[i].nonzero()[0]\n",
        "                if len(nonzero) <= num_features:\n",
        "                    break\n",
        "            used_features = nonzero\n",
        "            return used_features\n",
        "        elif method == 'auto':\n",
        "            if num_features <= 6:\n",
        "                n_method = 'forward_selection'\n",
        "            else:\n",
        "                n_method = 'highest_weights'\n",
        "            return self.feature_selection(data, labels, weights,\n",
        "                                          num_features, n_method)\n",
        "\n",
        "    def explain_instance_with_data(self,\n",
        "                                   neighborhood_data,\n",
        "                                   neighborhood_labels,\n",
        "                                   distances,\n",
        "                                   label,\n",
        "                                   num_features,\n",
        "                                   feature_selection='auto',\n",
        "                                   model_regressor=None):\n",
        "      \n",
        "        weights = self.kernel_fn(distances)\n",
        "        labels_column = neighborhood_labels[:, label]\n",
        "        used_features = self.feature_selection(neighborhood_data,\n",
        "                                               labels_column,\n",
        "                                               weights,\n",
        "                                               num_features,\n",
        "                                               feature_selection)\n",
        "        if model_regressor is None:\n",
        "            model_regressor = Ridge(alpha=1, fit_intercept=True,\n",
        "                                    random_state=self.random_state)\n",
        "        easy_model = model_regressor\n",
        "        easy_model.fit(neighborhood_data[:, used_features],\n",
        "                       labels_column, sample_weight=weights)\n",
        "        prediction_score = easy_model.score(\n",
        "            neighborhood_data[:, used_features],\n",
        "            labels_column, sample_weight=weights)\n",
        "\n",
        "        local_pred = easy_model.predict(neighborhood_data[0, used_features].reshape(1, -1))\n",
        "\n",
        "        if self.verbose:\n",
        "            print('Intercept', easy_model.intercept_)\n",
        "            print('Prediction_local', local_pred,)\n",
        "            print('Right:', neighborhood_labels[0, label])\n",
        "        return (easy_model.intercept_,\n",
        "                sorted(zip(used_features, easy_model.coef_),\n",
        "                       key=lambda x: np.abs(x[1]), reverse=True),\n",
        "                prediction_score, local_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qz5IIEsXF9k9"
      },
      "source": [
        "import copy\n",
        "from functools import partial\n",
        "\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.utils import check_random_state\n",
        "from skimage.color import gray2rgb\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ImageExplanation(object):\n",
        "    def __init__(self, image, segments):\n",
        "        self.image = image\n",
        "        self.segments = segments\n",
        "        self.intercept = {}\n",
        "        self.local_exp = {}\n",
        "        self.local_pred = {}\n",
        "        self.score = {}\n",
        "\n",
        "    def get_image_and_mask(self, label, positive_only=True, negative_only=False, hide_rest=False,\n",
        "                           num_features=5, min_weight=0.):\n",
        "        if label not in self.local_exp:\n",
        "            raise KeyError('Label not in explanation')\n",
        "        if positive_only & negative_only:\n",
        "            raise ValueError(\"Positive_only and negative_only cannot be true at the same time.\")\n",
        "        segments = self.segments\n",
        "        image = self.image\n",
        "        exp = self.local_exp[label]\n",
        "        mask = np.zeros(segments.shape, segments.dtype)\n",
        "        if hide_rest:\n",
        "            temp = np.zeros(self.image.shape)\n",
        "        else:\n",
        "            temp = self.image.copy()\n",
        "        if positive_only:\n",
        "            fs = [x[0] for x in exp\n",
        "                  if x[1] > 0 and x[1] > min_weight][:num_features]\n",
        "        if negative_only:\n",
        "            fs = [x[0] for x in exp\n",
        "                  if x[1] < 0 and abs(x[1]) > min_weight][:num_features]\n",
        "        if positive_only or negative_only:\n",
        "            for f in fs:\n",
        "                temp[segments == f] = image[segments == f].copy()\n",
        "                mask[segments == f] = 1\n",
        "            return temp, mask\n",
        "        else:\n",
        "            for f, w in exp[:num_features]:\n",
        "                if np.abs(w) < min_weight:\n",
        "                    continue\n",
        "                c = 0 if w < 0 else 1\n",
        "                mask[segments == f] = -1 if w < 0 else 1\n",
        "                temp[segments == f] = image[segments == f].copy()\n",
        "                temp[segments == f, c] = np.max(image)\n",
        "            return temp, mask\n",
        "\n",
        "\n",
        "class LimeImageExplainer(object):\n",
        "    def __init__(self, kernel_width=.25, kernel=None, verbose=False,\n",
        "                 feature_selection='auto', random_state=None):\n",
        "        kernel_width = float(kernel_width)\n",
        "\n",
        "        if kernel is None:\n",
        "            def kernel(d, kernel_width):\n",
        "                return np.sqrt(np.exp(-(d ** 2) / kernel_width ** 2))\n",
        "\n",
        "        kernel_fn = partial(kernel, kernel_width=kernel_width)\n",
        "\n",
        "        self.random_state = check_random_state(random_state)\n",
        "        self.feature_selection = feature_selection\n",
        "        self.base = LimeBase(kernel_fn, verbose, random_state=self.random_state)\n",
        "\n",
        "    def explain_instance(self, image, classifier_fn, labels=(1,),\n",
        "                         hide_color=None,\n",
        "                         top_labels=5, num_features=100000, num_samples=1000,\n",
        "                         batch_size=10,\n",
        "                         segmentation_fn=None,\n",
        "                         distance_metric='cosine',\n",
        "                         model_regressor=None,\n",
        "                         random_seed=None,\n",
        "                         progress_bar=True):\n",
        "        \n",
        "        if len(image.shape) == 2:\n",
        "            image = gray2rgb(image)\n",
        "        if random_seed is None:\n",
        "            random_seed = self.random_state.randint(0, high=1000)\n",
        "\n",
        "        if segmentation_fn is None:\n",
        "            segmentation_fn = SegmentationAlgorithm('quickshift', kernel_size=4,\n",
        "                                                    max_dist=200, ratio=0.2,\n",
        "                                                    random_seed=random_seed)\n",
        "        segments = segmentation_fn(image)\n",
        "\n",
        "        fudged_image = image.copy()\n",
        "        if hide_color is None:\n",
        "            for x in np.unique(segments):\n",
        "                fudged_image[segments == x] = (\n",
        "                    np.mean(image[segments == x][:, 0]),\n",
        "                    np.mean(image[segments == x][:, 1]),\n",
        "                    np.mean(image[segments == x][:, 2]))\n",
        "        else:\n",
        "            fudged_image[:] = hide_color\n",
        "\n",
        "        top = labels\n",
        "\n",
        "        data, labels = self.data_labels(image, fudged_image, segments,\n",
        "                                        classifier_fn, num_samples,\n",
        "                                        batch_size=batch_size,\n",
        "                                        progress_bar=progress_bar)\n",
        "\n",
        "        distances = sklearn.metrics.pairwise_distances(\n",
        "            data,\n",
        "            data[0].reshape(1, -1),\n",
        "            metric=distance_metric\n",
        "        ).ravel()\n",
        "\n",
        "        ret_exp = ImageExplanation(image, segments)\n",
        "        if top_labels:\n",
        "            top = np.argsort(labels[0])[-top_labels:]\n",
        "            ret_exp.top_labels = list(top)\n",
        "            ret_exp.top_labels.reverse()\n",
        "        for label in top:\n",
        "            (ret_exp.intercept[label],\n",
        "             ret_exp.local_exp[label],\n",
        "             ret_exp.score[label],\n",
        "             ret_exp.local_pred[label]) = self.base.explain_instance_with_data(\n",
        "                data, labels, distances, label, num_features,\n",
        "                model_regressor=model_regressor,\n",
        "                feature_selection=self.feature_selection)\n",
        "        return ret_exp\n",
        "\n",
        "    def data_labels(self,\n",
        "                    image,\n",
        "                    fudged_image,\n",
        "                    segments,\n",
        "                    classifier_fn,\n",
        "                    num_samples,\n",
        "                    batch_size=10,\n",
        "                    progress_bar=True):\n",
        "    \n",
        "        n_features = np.unique(segments).shape[0]\n",
        "        data = self.random_state.randint(0, 2, num_samples * n_features)\\\n",
        "            .reshape((num_samples, n_features))\n",
        "        labels = []\n",
        "        data[0, :] = 1\n",
        "        imgs = []\n",
        "        rows = tqdm(data) if progress_bar else data\n",
        "        for row in rows:\n",
        "            temp = copy.deepcopy(image)\n",
        "            zeros = np.where(row == 0)[0]\n",
        "            mask = np.zeros(segments.shape).astype(bool)\n",
        "            for z in zeros:\n",
        "                mask[segments == z] = True\n",
        "            temp[mask] = fudged_image[mask]\n",
        "            imgs.append(temp)\n",
        "            if len(imgs) == batch_size:\n",
        "                preds = classifier_fn(np.array(imgs))\n",
        "                labels.extend(preds)\n",
        "                imgs = []\n",
        "        if len(imgs) > 0:\n",
        "            preds = classifier_fn(np.array(imgs))\n",
        "            labels.extend(preds)\n",
        "        return data, np.array(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNyejdHFH7P0"
      },
      "source": [
        "def get_pil_transform(): \n",
        "    transf = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.CenterCrop(224)\n",
        "    ])    \n",
        "\n",
        "    return transf\n",
        "\n",
        "def get_preprocess_transform():\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                    std=[0.229, 0.224, 0.225])     \n",
        "    transf = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])    \n",
        "\n",
        "    return transf    \n",
        "\n",
        "pill_transf = get_pil_transform()\n",
        "preprocess_transform = get_preprocess_transform()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ic2skaFLDM8"
      },
      "source": [
        "# Explaination"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "543jT4-LIi8V",
        "outputId": "112ae7fd-e0da-43be-fee5-64354c170839"
      },
      "source": [
        "images = test_set.data\n",
        "labels = test_set.targets\n",
        "img = images[0,:,:]\n",
        "images.shape\n",
        "plt.imshow(img)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f6c9d5bcfd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dxx3vgZ9J3CH"
      },
      "source": [
        "def batch_predict(images):\n",
        "    net.eval()\n",
        "    batch = torch.stack(tuple(preprocess_transform(i) for i in images), dim=0)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    net.to(device)\n",
        "    batch = batch.to(device)\n",
        "    batch=  batch[:,:1,:,:]\n",
        "    logits = net(batch)\n",
        "    probs = F.softmax(logits, dim=1)\n",
        "    return probs.detach().cpu().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhM93S4zO8pV"
      },
      "source": [
        "from skimage.segmentation import mark_boundaries\n",
        "import random\n",
        "\n",
        "\n",
        "segmentation_fn = SegmentationAlgorithm('quickshift', kernel_size=2,\n",
        "                                          max_dist=2, ratio=0.2,\n",
        "                                          random_seed= int(random.random()*1000))\n",
        "\n",
        "\n",
        "# segmentation_fn= SegmentationAlgorithm('felzenszwalb', scale=1.0, sigma=0.3)\n",
        "# segmentation_fn= SegmentationAlgorithm('slic', n_segments=100, compactness=0.1)\n",
        "\n",
        "\n",
        "def explain(img,show=False):\n",
        "  if show:\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "  explainer = LimeImageExplainer()\n",
        "  explanation = explainer.explain_instance(img, \n",
        "                                         batch_predict, # classification function\n",
        "                                         top_labels=1, \n",
        "                                        #  hide_color=0, \n",
        "                                        segmentation_fn = segmentation_fn,\n",
        "                                         num_samples=100 )\n",
        "\n",
        "  temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only= False, num_features=20, hide_rest=False)\n",
        "  img_boundry1 = mark_boundaries(temp, mask, color=(1, 1, 0))\n",
        "  if show:\n",
        "    # print(i, np.mean(mask), explanation.top_labels[0])\n",
        "    print(explanation.local_exp[explanation.top_labels[0]])\n",
        "    # print(explanation.segments)\n",
        "    plt.imshow(img_boundry1)\n",
        "    \n",
        "    plt.show()\n",
        "    plt.imshow(mask)\n",
        "    plt.show()\n",
        "  return img_boundry1, explanation.segments, explanation.local_exp[explanation.top_labels[0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7rEiSIOu8Zw"
      },
      "source": [
        "import random\n",
        "from google.colab import files\n",
        "\n",
        "N = np.shape(images)[0]\n",
        "samples = []\n",
        "indexes = []\n",
        "for i in range(10):\n",
        "  a =  [j for j in range(N) if labels[j]==i]\n",
        "  random.shuffle(a) \n",
        "  indexes.extend(a[:3])\n",
        "\n",
        "# indexes = np.random.permutation(np.shape(images)[0])\n",
        "# print(len(indexes))\n",
        "import os\n",
        "os.mkdir('result')\n",
        "for i in range(len(indexes)):\n",
        "  img = images[indexes[i],:,:]\n",
        "  img_boundry, segs, epx = explain(img, show=False)\n",
        "  \n",
        "  plt.imshow(img)\n",
        "  label = int(i/3)\n",
        "  plt.savefig('result/mnist_'+str(label)+'_lime_original_'+str(i%3)+'.png')\n",
        "  # files.download('result/mnist_'+str(label)+'_lime_original_'+str(i%3)+'.png')\n",
        "  plt.show()\n",
        "  plt.imshow(img_boundry)\n",
        "  plt.savefig('result/mnist_'+str(label)+'_lime_explain_'+str(i%3)+'.png')\n",
        "  # files.download('result/mnist_'+str(label)+'_lime_explain_'+str(i%3)+'.png')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsvzkEM-S8gf"
      },
      "source": [
        "! zip -r mnist_lime.zip result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "MdhOXe3_Tb4W",
        "outputId": "2734aa52-f2e3-42de-e7cf-592545bdf9e7"
      },
      "source": [
        "files.download('mnist_lime.zip')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_465d489d-cb29-46c6-b597-0868c085067d\", \"mnist_lime.zip\", 236927)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP1JTvCQk1ad"
      },
      "source": [
        "def only_keep_important(img, mask, exp, ratio=0.2):\n",
        "  m, n = np.shape(mask)\n",
        "  N = int((1-ratio) * m * n)\n",
        "  imgg = img.clone()\n",
        "  # print(np.shape(imgg))\n",
        "  exp = sorted(exp, key=lambda x: x[1], reverse=True)\n",
        "  # print(exp,len(exp))\n",
        "  for idx in range(len(exp)):\n",
        "    i,j = exp[idx]\n",
        "    if j>= 0:\n",
        "      mask_ = mask.copy()\n",
        "      \n",
        "      k = len(mask_[mask_ == i])\n",
        "      mask_[mask_ == i] = 0\n",
        "      mask_[mask != i] = 1\n",
        "      # print(np.shape(mask_),np.shape(imgg))\n",
        "      # mask_ = np.concatenate([mask_,mask_,mask_])\n",
        "      # mask_ = mask_.reshape(32,32,3)\n",
        "      imgg = imgg * mask_ \n",
        "      \n",
        "      # print(mask_)\n",
        "      # print(imgg)\n",
        "      # print(N,k,i)\n",
        "      if N <= k:\n",
        "        # print('breaking')\n",
        "        break\n",
        "      N -=k\n",
        "    \n",
        "  return imgg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jv-_dl1K6W2k"
      },
      "source": [
        "## Effect of pixels\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdcDCLo4GIdc"
      },
      "source": [
        "import time\n",
        "N = np.shape(images)[0]\n",
        "print(N)\n",
        "new_images = []\n",
        "t = time.time()\n",
        "for i in range(2000):\n",
        "  img = images[i,:,:]\n",
        "  img_boundry, mask, exp = explain(img,show=False)\n",
        "  new_img = only_keep_important(img, mask, exp)\n",
        "  new_images.append([new_img.reshape(1,28,28), labels[i]])\n",
        "print(time.time()-t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0heV6CJNb0y"
      },
      "source": [
        "new_testloader = torch.utils.data.DataLoader(new_images, batch_size=batch_size, shuffle=False, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYqgJ8k0PdZU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "116d4204-3b61-43ac-ae17-33518007ac53"
      },
      "source": [
        "net.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "# since we're not\\ training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in new_testloader:\n",
        "        images, labels = data\n",
        "        # print(images.shape, labels.shape)\n",
        "        images = images.to(DEVICE).type(torch.cuda.FloatTensor)\n",
        "        labels = labels.to(DEVICE).type(torch.cuda.FloatTensor)\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = net(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "print('Accuracy of the network on the test images: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the test images: 57 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3X7o7Rxbvqu"
      },
      "source": [
        "# Latex Code Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bXyloKDbyg6",
        "outputId": "e3435dac-4865-464b-ce37-f3fefd864fa3"
      },
      "source": [
        "for k in range(5):\n",
        "  print('\\\\begin{figure}[htbp]')\n",
        "  print('\\\\centering')\n",
        "  for i in range(2):\n",
        "    index = 2*k+i\n",
        "    for j in range(3):\n",
        "      print('\\\\begin{minipage}[t]{0.4\\\\textwidth}')\n",
        "      print('\\\\centering')\n",
        "      print('\\\\includegraphics[width=0.9\\\\linewidth]{mnist_lime/mnist_'+str(index)+'_lime_original_'+ str(j) +'.png}')\n",
        "      # print('\\\\subcaption{Original images}')\n",
        "      # \\source{Autoria prpria.}\n",
        "      print('\\\\end{minipage}')\n",
        "      print('\\\\hfill')\n",
        "      print('\\\\begin{minipage}[t]{0.4\\\\textwidth}')\n",
        "      print('\\\\centering')\n",
        "      print('\\\\includegraphics[width=0.9\\\\linewidth]{mnist_lime/mnist_'+str(index)+'_lime_explain_'+ str(j) +'.png}')\n",
        "      # print('\\\\subcaption{LRP explanation of the image}')\n",
        "      print('\\\\end{minipage}')\n",
        "\n",
        "  print('\\\\caption{Original and Lime explanation of MNIST images. Images in left column are original images and images in the right column are LIME explanations.}')\n",
        "  print('\\\\end{figure}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\\begin{figure}[htbp]\n",
            "\\centering\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_0_lime_original_0.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_0_lime_explain_0.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_0_lime_original_1.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_0_lime_explain_1.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_0_lime_original_2.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_0_lime_explain_2.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_1_lime_original_0.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_1_lime_explain_0.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_1_lime_original_1.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_1_lime_explain_1.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_1_lime_original_2.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_1_lime_explain_2.png}\n",
            "\\end{minipage}\n",
            "\\caption{Original and Lime explanation of MNIST images. Images in left column are original images and images in the right column are LIME explanations.}\n",
            "\\end{figure}\n",
            "\\begin{figure}[htbp]\n",
            "\\centering\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_2_lime_original_0.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_2_lime_explain_0.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_2_lime_original_1.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_2_lime_explain_1.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_2_lime_original_2.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_2_lime_explain_2.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_3_lime_original_0.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_3_lime_explain_0.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_3_lime_original_1.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_3_lime_explain_1.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_3_lime_original_2.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_3_lime_explain_2.png}\n",
            "\\end{minipage}\n",
            "\\caption{Original and Lime explanation of MNIST images. Images in left column are original images and images in the right column are LIME explanations.}\n",
            "\\end{figure}\n",
            "\\begin{figure}[htbp]\n",
            "\\centering\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_4_lime_original_0.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_4_lime_explain_0.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_4_lime_original_1.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_4_lime_explain_1.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_4_lime_original_2.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_4_lime_explain_2.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_5_lime_original_0.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_5_lime_explain_0.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_5_lime_original_1.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_5_lime_explain_1.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_5_lime_original_2.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_5_lime_explain_2.png}\n",
            "\\end{minipage}\n",
            "\\caption{Original and Lime explanation of MNIST images. Images in left column are original images and images in the right column are LIME explanations.}\n",
            "\\end{figure}\n",
            "\\begin{figure}[htbp]\n",
            "\\centering\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_6_lime_original_0.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_6_lime_explain_0.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_6_lime_original_1.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_6_lime_explain_1.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_6_lime_original_2.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_6_lime_explain_2.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_7_lime_original_0.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_7_lime_explain_0.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_7_lime_original_1.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_7_lime_explain_1.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_7_lime_original_2.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_7_lime_explain_2.png}\n",
            "\\end{minipage}\n",
            "\\caption{Original and Lime explanation of MNIST images. Images in left column are original images and images in the right column are LIME explanations.}\n",
            "\\end{figure}\n",
            "\\begin{figure}[htbp]\n",
            "\\centering\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_8_lime_original_0.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_8_lime_explain_0.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_8_lime_original_1.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_8_lime_explain_1.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_8_lime_original_2.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_8_lime_explain_2.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_9_lime_original_0.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_9_lime_explain_0.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_9_lime_original_1.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_9_lime_explain_1.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_9_lime_original_2.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{mnist_lime/mnist_9_lime_explain_2.png}\n",
            "\\end{minipage}\n",
            "\\caption{Original and Lime explanation of MNIST images. Images in left column are original images and images in the right column are LIME explanations.}\n",
            "\\end{figure}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xR_smQbm50PO"
      },
      "source": [
        "# Model Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORs7bj_065S4",
        "outputId": "f5fda3c2-7c05-46d3-8a25-baf98084c32a"
      },
      "source": [
        "!pip install hiddenlayer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hiddenlayer\n",
            "  Downloading https://files.pythonhosted.org/packages/64/f8/ea51d02695a4dc397f3b2487fae462cd3f2ce707c54250e0fdfaec2ff92e/hiddenlayer-0.3-py3-none-any.whl\n",
            "Installing collected packages: hiddenlayer\n",
            "Successfully installed hiddenlayer-0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDTLLCnK52hl",
        "outputId": "3154b1f7-c4a1-47c7-a454-381ceea5dd8d"
      },
      "source": [
        "import hiddenlayer as hl\n",
        "\n",
        "transforms = [ hl.transforms.Prune('Constant') ] # Removes Constant nodes from graph.\n",
        "\n",
        "graph = hl.build_graph(net, torch.tensor(np.random.rand(10,1,28,28),device='cuda').type(torch.cuda.FloatTensor), transforms=transforms)\n",
        "graph.theme = hl.graph.THEMES['blue'].copy()\n",
        "graph.save('rnn_hiddenlayer', format='png')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/onnx/symbolic_helper.py:715: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
            "  training_mode + \", as specified by the export mode.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-LUCd2x_XmJ",
        "outputId": "dfa7882a-d0da-4811-fba8-666905ee1b4c"
      },
      "source": [
        "!pip install torchviz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchviz\n",
            "  Downloading https://files.pythonhosted.org/packages/79/e7/643808913211d6c1fc96a3a4333bf4c9276858fab00bcafaf98ea58a97be/torchviz-0.0.2.tar.gz\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchviz) (1.9.0+cu102)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (3.7.4.3)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.2-cp37-none-any.whl size=4152 sha256=53780610d97e8f70c31632ef263719618b8b43a35ac84ad1514e16a64ce706f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/26/58/026ffd533dbe8b3972eb423da9c7949beca68d1c98ed9e8624\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ezVssfpU_X2J",
        "outputId": "8198134a-14f3-4f37-8148-2ac85a0f925e"
      },
      "source": [
        "from torchviz import make_dot\n",
        "\n",
        "make_dot(net(torch.tensor(np.random.rand(128,1,28,28),device='cuda').type(torch.cuda.FloatTensor)), params=dict(list(net.named_parameters()))).render(\"rnn_torchviz\", format=\"png\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'rnn_torchviz.png'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    }
  ]
}